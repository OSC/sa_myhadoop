#!/usr/bin/env perl

use strict;
use warnings;
use XML::Simple;
use File::Spec;
use File::Path;

### define some reasonable values as universal defaults
my $myhadoop_config = {
    nodes           => 2,
    persist         => "",  # either contains base_dir or null for no persistent hdfs
    hadoop_conf_dir => File::Spec::catfile( $ENV{'HOME'}, "config" )
    hadoop_home     => ( $ENV{'HADOOP_HOME'} || $ENV{'HADOOP_PREFIX'} )
    my_hadoop_home  => File::Spec::rel2abs($0),
    hadoop_data_dir => '/tmp',
    hadoop_log_dir  => '/tmp',
    hadoop_pid_dir  => '/tmp',
    hadoop_tmp_dir  => '/tmp',

};
$myhadoop_config->{'hadoop_conf_templates'} = 
    File::Spec::catfile( $myhadoop_config->{'hadoop_home'}, 'conf' );

### read in site-specific configuration here

### prepare our local config dir
$myhadoop_config->{hadoop_conf_dir} = 
    File::Spec::rel2abs($myhadoop_config->{hadoop_conf_dir});
eval {
    if ( -d $myhadoop_config->{hadoop_conf_dir} ) {
        my $backup_dir = sprintf( "%s.bak", $myhadoop_config->{hadoop_conf_dir});
        File::Path::rmtree( $backup_dir ) if -d $backup_dir;
        move( $myhadoop_config->{hadoop_conf_dir}, $backup_dir )
    }
    File::Path::make_path( $myhadoop_config->{hadoop_conf_dir}, { mode => '0700' } );
};
die $@ if $@;

### Process all the template configs
my $hadoop_conf_overrides = { 
    ### universal configuration
    'core-site.xml' => {
        'fs.default.name' => {
            'value' => sprintf("hdfs://%s:%d",
                $myhadoop_config->{'namenode_host'},
                $myhadoop_config->{'namenode_port'}),
            'description' => 'The name of the default file system',
        },
        'hadoop.tmp.dir' => {
            'value' => $myhadoop_config->{'hadoop_tmp_dir'},
            'description' => 'A base for other temporary directories',
        },
    },

    ### hdfs-specific options
    'hdfs-site.xml' => {
    },

    ### mapreduce framework's options
    'mapred-site.xml' => {
        'mapred.job.tracker' => {
            'value' => sprintf( "%s:%d",
                $myhadoop_config->{'jobtracker_host'},
                $myhadoop_config->{'jobtracker_port'}),
            'description' => 'The host and port of the MapReduce job tracker',
        },
    },
};

### Add user-defined overrides here

### Merge overrides into templates, then write out to our local conf dir
foreach my $config_file ( keys(%$hadoop_config_templates) )
{
    my $input_file = File::Spec::catfile( 
        $myhadoop_config->{'hadoop_conf_templates'}, 
        $config_file );
    my $output_file = File::Spec::catfile( 
        $myhadoop_config->{'hadoop_conf_dir'}, 
        $config_file );

    my $input_xml = XMLin($input_file)->{'property'};

    my $output_xml->{'property'} = merge_hashes( $input_xml, 
        $hadoop_config_overrides->{$config_file} );

    open OUTPUTXML, ">$output_file" or die $@;
    print OUTPUTXML XMLout($output_xml,  
    'RootName'  => 'configuration', 
    'NoAttr'    =>  1,
    'XMLDecl'   =>  '<?xml version="1.0"?>' ."\n".
        '<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>' );
    close OUTPUTXML;
}

### define the master in $HADOOP_CONF_DIR/masters
### define the slave in $HADOOP_CONF_DIR/slaves

### populate hadoop-env.sh
###   add "export HADOOP_LOG_DIR=$HADOOP_LOG_DIR" to hadoop-env.sh
###   ...and just export all env variables to mappers/reducers?

# for ((i=1; i<=$NODES; i++))
# do
#     node=`awk 'NR=='"$i"'{print;exit}' $PBS_NODEFILE`
#     echo "Configuring node: $node"
#     cmd="rm -rf $HADOOP_LOG_DIR; mkdir -p $HADOOP_LOG_DIR"
#     echo $cmd
#     ssh $node $cmd 
#     if [ "$PERSIST" = "true" ]; then
#         cmd="rm -rf $HADOOP_DATA_DIR; ln -s $BASE_DIR/$i $HADOOP_DATA_DIR"
#         echo $cmd
#         ssh $node $cmd
#     else
#         cmd="rm -rf $HADOOP_DATA_DIR; mkdir -p $HADOOP_DATA_DIR"
#         echo $cmd
#         ssh $node $cmd 
#     fi
# done

# $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR namenode -format
 
# $HADOOP_HOME/bin/start-all.sh
 
# sleep $((12*3600-180))
 
# $HADOOP_HOME/bin/stop-all.sh
# cp -Lr $HADOOP_LOG_DIR $PBS_O_WORKDIR/hadoop-logs.$PBS_JOBID

# $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR namenode -format
 
# $HADOOP_HOME/bin/start-all.sh
 
# sleep $((12*3600-180))
 
# $HADOOP_HOME/bin/stop-all.sh
# cp -Lr $HADOOP_LOG_DIR $PBS_O_WORKDIR/hadoop-logs.$PBS_JOBID

################################################################################
sub merge_hashes{
    my $input_hash = shift;
    my $overrides = shift;

    my $new_hash = $input_hash;
    $new_hash->{$_} = $overrides->{$_} foreach keys($overrides);

    return $new_hash;
}
